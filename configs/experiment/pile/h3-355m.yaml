# @package _global_
defaults:
  - /experiment/pile/h3.yaml
  - /experiment/pile/355m

model:
  attn_layer_idx: [1, 13]
  attn_cfg:
    num_heads: 16
    use_flash_attn: True
    fused_bias_fc: True
    dropout: 0.1

optimizer:
  lr: 1e-4
  weight_decay: 0.01

trainer:
  accelerator: gpu
  devices: 2
  num_nodes: 1
  accumulate_grad_batches: ${div_up:${train.global_batch_size}, ${eval:${trainer.devices} * ${loader.batch_size} * ${trainer.num_nodes}}}
  max_epochs: 5
